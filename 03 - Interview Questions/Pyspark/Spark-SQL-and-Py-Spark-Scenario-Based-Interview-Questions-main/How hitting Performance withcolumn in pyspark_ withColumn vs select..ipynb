{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a05ef420-a8dd-426f-856f-d8df31507293",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Using the withColumn method in PySpark can impact performance, especially if used multiple times within the same operation. Here's why this happens and some tips to optimize your code.\n",
    "\n",
    "Why withColumn Can Impact Performance\n",
    "DataFrame Immutability: PySpark DataFrames are immutable, meaning every time you use withColumn, it creates a new DataFrame with the added or modified column. If you chain multiple withColumn operations, PySpark will create multiple intermediate DataFrames, which can lead to unnecessary overhead.\n",
    "\n",
    "Re-computation: Each withColumn operation can potentially trigger a re-computation of the entire DataFrame lineage if not properly cached, which adds to the computation time.\n",
    "\n",
    "Job Overhead: Each withColumn can translate to additional jobs and stages, adding overhead in job planning and execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8de76e0-f7be-4ce9-9940-3573b90d3033",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"withColumn Performance\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "df = spark.range(0, 100)\n",
    "\n",
    "# Adding multiple columns with separate withColumn calls\n",
    "df = df.withColumn(\"col1\", col(\"id\") + 1)\n",
    "df = df.withColumn(\"col2\", col(\"id\") + 2)\n",
    "df = df.withColumn(\"col3\", col(\"id\") + 3)\n",
    "df = df.withColumn(\"col4\", col(\"id\") + 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e71d25cd-807f-4fd5-b57b-3cf33a3fc099",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Optimized Approach\n",
    "Single select or withColumn Call: Instead of chaining multiple withColumn calls, combine them in a single select or withColumn call.\n",
    "\n",
    "Temporary Variables: Use temporary variables to hold intermediate results, reducing re-computation.\n",
    "\n",
    "Cache Intermediate DataFrames: Cache DataFrames when you need to perform multiple transformations to avoid re-computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efb41e3a-6865-4ad9-ad4b-0681227d6546",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Single select call to add multiple columns\n",
    "df_optimized = df.select(\n",
    "    col(\"id\"),\n",
    "    (col(\"id\") + 1).alias(\"col1\"),\n",
    "    (col(\"id\") + 2).alias(\"col2\"),\n",
    "    (col(\"id\") + 3).alias(\"col3\"),\n",
    "    (col(\"id\") + 4).alias(\"col4\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d64e6cb8-6da7-4d43-8dd7-cc9c21423a8c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Comparison\n",
    "\n",
    "Using Multiple \"withColumn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c7e7bd3-6ccb-4704-b176-16708c44c839",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken with multiple withColumn calls: 1.0663623809814453 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Adding multiple columns with separate withColumn calls\n",
    "df = spark.range(0, 1000000)\n",
    "df = df.withColumn(\"col1\", col(\"id\") + 1)\n",
    "df = df.withColumn(\"col2\", col(\"id\") + 2)\n",
    "df = df.withColumn(\"col3\", col(\"id\") + 3)\n",
    "df = df.withColumn(\"col4\", col(\"id\") + 4)\n",
    "\n",
    "# Trigger an action to materialize the transformations\n",
    "df.count()\n",
    "\n",
    "print(\"Time taken with multiple withColumn calls: {} seconds\".format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e06b9fa9-202c-4d54-ad6b-29493367f4cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Using Single \"select\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01f375b3-d123-42f9-829e-87e001b7a35a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken with single select call: 0.6064820289611816 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Adding multiple columns in a single select call\n",
    "df = spark.range(0, 1000000)\n",
    "df_optimized = df.select(\n",
    "    col(\"id\"),\n",
    "    (col(\"id\") + 1).alias(\"col1\"),\n",
    "    (col(\"id\") + 2).alias(\"col2\"),\n",
    "    (col(\"id\") + 3).alias(\"col3\"),\n",
    "    (col(\"id\") + 4).alias(\"col4\")\n",
    ")\n",
    "\n",
    "# Trigger an action to materialize the transformations\n",
    "df_optimized.count()\n",
    "\n",
    "print(\"Time taken with single select call: {} seconds\".format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fef9ab3-c95d-4159-b9bc-d5313c8d95a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Conclusion\n",
    "\n",
    "Using withColumn multiple times can degrade performance due to DataFrame immutability, re-computation, and job overhead. To optimize:\n",
    "\n",
    "Combine multiple column additions in a single select or withColumn call.\n",
    "Use temporary variables to store intermediate results.\n",
    "Cache DataFrames when performing multiple transformations.\n",
    "By following these practices, you can significantly improve the performance of your PySpark jobs."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "How hitting Performance withcolumn in pyspark| withColumn vs select.",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
