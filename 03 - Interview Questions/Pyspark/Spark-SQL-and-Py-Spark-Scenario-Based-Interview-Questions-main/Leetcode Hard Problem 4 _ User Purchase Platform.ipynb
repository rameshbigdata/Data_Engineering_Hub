{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e518ea5-6f6a-4508-9e56-af39ac6599d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Problem Statement\n",
    "product_id is the primary key for this table. \n",
    "period_start and period_end indicates the start and end date for sales period, both dates are inclusive.\n",
    "The average_daily_sales column holds the average daily sales amount of the items for the period.\n",
    "Write an SQL query to report the Total sales amount of each item for each year, with corresponding product name, product_id, product_name and report_year.\n",
    "\n",
    "Dates of the sales years are between 2018 to 2020. Return the result table ordered by product_id and report_year.\n",
    "\n",
    "The query result format is in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfb68dce-227b-46ba-8ebb-8c7c5920568a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------------------+\n|product_id|period_start|period_end|average_daily_sales|\n+----------+------------+----------+-------------------+\n|         1|  2019-01-25|2019-02-28|                100|\n|         2|  2018-12-01|2020-01-01|                 10|\n|         3|  2019-12-01|2020-01-31|                  1|\n+----------+------------+----------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, sequence, year, col, expr\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CreateSalesTable\").getOrCreate()\n",
    "\n",
    "# Define the schema for the sales table\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"period_start\", DateType(), True),\n",
    "    StructField(\"period_end\", DateType(), True),\n",
    "    StructField(\"average_daily_sales\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create data to be inserted into the sales table with datetime.date objects\n",
    "data = [\n",
    "    (1, datetime.strptime('2019-01-25', '%Y-%m-%d').date(),\n",
    "        datetime.strptime('2019-02-28', '%Y-%m-%d').date(), 100),\n",
    "    (2, datetime.strptime('2018-12-01', '%Y-%m-%d').date(),\n",
    "        datetime.strptime('2020-01-01', '%Y-%m-%d').date(), 10),\n",
    "    (3, datetime.strptime('2019-12-01', '%Y-%m-%d').date(),\n",
    "        datetime.strptime('2020-01-31', '%Y-%m-%d').date(), 1)\n",
    "]\n",
    "\n",
    "# Convert data to DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acbe8086-8b7f-47da-9a07-e6b55c3d0da0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0a81b7d-6590-499a-9ca5-8ed820ee553c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------+\n|product_id|report_year|total_amount|\n+----------+-----------+------------+\n|         1|       2019|        3500|\n|         2|       2018|         310|\n|         2|       2019|        3650|\n|         2|       2020|          10|\n|         3|       2019|          31|\n|         3|       2020|          31|\n+----------+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Generate a DataFrame with each day in the period\n",
    "expanded_df = df.withColumn(\n",
    "    \"day\", explode(sequence(col(\"period_start\"), col(\"period_end\")))\n",
    ")\n",
    "\n",
    "# Calculate total sales per year\n",
    "result_df = (\n",
    "    expanded_df.withColumn(\"report_year\", year(col(\"day\")))\n",
    "    .groupBy(\"product_id\", \"report_year\")\n",
    "    .agg(expr(\"sum(average_daily_sales) as total_amount\"))\n",
    ")\n",
    "\n",
    "# Show the result DataFrame\n",
    "result_df.orderBy(\"product_id\", \"report_year\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59a15de8-f75c-4eb3-b792-bc56a3c1a1cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1ca4d8-f18e-4464-8ae5-e399e5347210",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69343bdf-72f4-4e89-b00b-72a24914bab7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------+\n|product_id|report_year|total_amount|\n+----------+-----------+------------+\n|         1|       2019|        3500|\n|         2|       2018|         310|\n|         2|       2019|        3650|\n|         2|       2020|          10|\n|         3|       2019|          31|\n|         3|       2020|          31|\n+----------+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Use Spark SQL to perform the required operations\n",
    "result_df = spark.sql(\n",
    "    \"\"\"\n",
    "    WITH all_dates AS (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            period_start,\n",
    "            period_end,\n",
    "            average_daily_sales,\n",
    "            explode(sequence(period_start, period_end, interval 1 day)) AS day\n",
    "        FROM sales\n",
    "    ),\n",
    "    daily_sales AS (\n",
    "        SELECT\n",
    "            product_id,\n",
    "            year(day) AS report_year,\n",
    "            average_daily_sales\n",
    "        FROM all_dates\n",
    "    )\n",
    "    SELECT\n",
    "        product_id,\n",
    "        report_year,\n",
    "        SUM(average_daily_sales) AS total_amount\n",
    "    FROM daily_sales\n",
    "    GROUP BY product_id, report_year\n",
    "    ORDER BY product_id, report_year\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Show the result DataFrame\n",
    "result_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3001123161078326,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Leetcode Hard Problem 4 | User Purchase Platform",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
