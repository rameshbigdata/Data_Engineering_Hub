{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cbb4109-71f5-418e-ab9d-e5425704f200",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Cache vs. Persist\n",
    "In Databricks and Apache Spark, both cache and persist are used to store DataFrames or RDDs in memory to speed up subsequent actions or transformations. However, they have different use cases and implications for performance. Hereâ€™s a detailed comparison and explanation of when to use each.\n",
    "\n",
    "\n",
    "###Cache:\n",
    "\n",
    "Default Storage Level: MEMORY_ONLY\n",
    "Purpose: Quickly store the DataFrame/RDD in memory.\n",
    "Syntax: df.cache()\n",
    "Use Case: When you want to store the data in memory for quick access and when memory is sufficient to hold the entire dataset.\n",
    "\n",
    "###Persist:\n",
    "\n",
    "Custom Storage Levels: Allows specifying different storage levels (e.g., MEMORY_AND_DISK, DISK_ONLY, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER).\n",
    "Purpose: Store the DataFrame/RDD using different storage levels, allowing for more flexibility.\n",
    "Syntax: df.persist(storageLevel)\n",
    "Use Case: When you need to customize the storage level based on the available resources and the size of the data. Useful for larger datasets that might not fit entirely in memory.\n",
    "Storage Levels\n",
    "MEMORY_ONLY: Stores RDD/DataFrame as deserialized Java objects in the JVM. If it does not fit in memory, some partitions will not be cached and will need to be recomputed when accessed again.\n",
    "MEMORY_AND_DISK: Stores RDD/DataFrame as deserialized Java objects in memory. If it does not fit in memory, partitions are stored on disk.\n",
    "DISK_ONLY: Stores RDD/DataFrame only on disk.\n",
    "MEMORY_ONLY_SER: Stores RDD/DataFrame as serialized Java objects (one byte array per partition). This is more space-efficient than deserialized objects, but more CPU intensive to read.\n",
    "MEMORY_AND_DISK_SER: Similar to MEMORY_ONLY_SER, but spills partitions to disk when they do not fit in memory.\n",
    "OFF_HEAP: (Experimental) Stores RDD/DataFrame in serialized format in Tachyon, a distributed storage system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30ce653e-baf7-4b78-a377-07672bffccf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storage level after cache:  Disk Memory Deserialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Cache vs Persist\").getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = spark.range(0, 100)\n",
    "\n",
    "# Cache the DataFrame\n",
    "df_cached = df.cache()\n",
    "\n",
    "# Trigger an action to materialize the cache\n",
    "df_cached.count()\n",
    "\n",
    "# Check storage level\n",
    "print(\"Storage level after cache: \", df_cached.storageLevel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adfdd436-e2fe-4d2f-adb4-3d23c3a09431",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storage level after persist:  Disk Memory Deserialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# Persist the DataFrame with MEMORY_AND_DISK\n",
    "df_persisted = df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Trigger an action to materialize the persist\n",
    "df_persisted.count()\n",
    "\n",
    "# Check storage level\n",
    "print(\"Storage level after persist: \", df_persisted.storageLevel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77378c44-be57-4a83-982f-4d3346ffd6c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Performance Implications:\n",
    "\n",
    "Memory Usage: \n",
    "\n",
    "cache uses the MEMORY_ONLY storage level, which might lead to OutOfMemory errors if the dataset is large. persist with MEMORY_AND_DISK is safer for large datasets, as it spills to disk when memory is insufficient.\n",
    "\n",
    "Execution Speed: \n",
    "\n",
    "Storing data in memory is faster than on disk. cache can be faster if the dataset fits in memory. persist with disk storage can be slower but prevents recomputation.\n",
    "\n",
    "Flexibility: \n",
    "persist offers more flexibility with multiple storage levels to balance between memory usage and execution speed.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Use cache for quick and easy storage in memory when the dataset fits comfortably within available memory.\n",
    "Use persist when you need more control over the storage level, especially for larger datasets or when working with limited memory resources.\n",
    "Choosing the right method and storage level depends on your specific use case, data size, and available resources. Proper use of caching and persistence can significantly improve the performance of your Spark jobs on Databricks."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks | Spark | Cache vs Persist",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
