{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1224bf30-1f13-4aa1-9378-626943de5233",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>empid</th></tr></thead><tbody><tr><td>2024-01-13T09:25:00.000+0000</td><td>10</td></tr><tr><td>2024-01-13T19:35:00.000+0000</td><td>10</td></tr><tr><td>2024-01-16T09:10:00.000+0000</td><td>10</td></tr><tr><td>2024-01-16T18:10:00.000+0000</td><td>10</td></tr><tr><td>2024-02-11T09:07:00.000+0000</td><td>10</td></tr><tr><td>2024-02-11T19:20:00.000+0000</td><td>10</td></tr><tr><td>2024-02-17T08:40:00.000+0000</td><td>17</td></tr><tr><td>2024-02-17T18:04:00.000+0000</td><td>17</td></tr><tr><td>2024-03-23T09:20:00.000+0000</td><td>10</td></tr><tr><td>2024-03-23T18:30:00.000+0000</td><td>10</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2024-01-13T09:25:00.000+0000",
         10
        ],
        [
         "2024-01-13T19:35:00.000+0000",
         10
        ],
        [
         "2024-01-16T09:10:00.000+0000",
         10
        ],
        [
         "2024-01-16T18:10:00.000+0000",
         10
        ],
        [
         "2024-02-11T09:07:00.000+0000",
         10
        ],
        [
         "2024-02-11T19:20:00.000+0000",
         10
        ],
        [
         "2024-02-17T08:40:00.000+0000",
         17
        ],
        [
         "2024-02-17T18:04:00.000+0000",
         17
        ],
        [
         "2024-03-23T09:20:00.000+0000",
         10
        ],
        [
         "2024-03-23T18:30:00.000+0000",
         10
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "empid",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, IntegerType\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, lag, lead, date_format, coalesce, expr\n",
    "from pyspark.sql.functions import row_number, date_format, unix_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Insert into emp_tbl\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema for the emp_tbl table\n",
    "schema = StructType([\n",
    "    StructField(\"id\", TimestampType(), True),\n",
    "    StructField(\"empid\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for the emp_tbl table with datetime conversion\n",
    "data = [\n",
    "    (datetime.strptime('2024-01-13 09:25:00', '%Y-%m-%d %H:%M:%S'), 10),\n",
    "    (datetime.strptime('2024-01-13 19:35:00', '%Y-%m-%d %H:%M:%S'), 10),\n",
    "    (datetime.strptime('2024-01-16 09:10:00', '%Y-%m-%d %H:%M:%S'), 10),\n",
    "    (datetime.strptime('2024-01-16 18:10:00', '%Y-%m-%d %H:%M:%S'), 10),\n",
    "    (datetime.strptime('2024-02-11 09:07:00', '%Y-%m-%d %H:%M:%S'), 10),\n",
    "    (datetime.strptime('2024-02-11 19:20:00', '%Y-%m-%d %H:%M:%S'), 10),\n",
    "    (datetime.strptime('2024-02-17 08:40:00', '%Y-%m-%d %H:%M:%S'), 17),\n",
    "    (datetime.strptime('2024-02-17 18:04:00', '%Y-%m-%d %H:%M:%S'), 17),\n",
    "    (datetime.strptime('2024-03-23 09:20:00', '%Y-%m-%d %H:%M:%S'), 10),\n",
    "    (datetime.strptime('2024-03-23 18:30:00', '%Y-%m-%d %H:%M:%S'), 10)\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the data and schema\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Create the emp_tbl in Spark SQL\n",
    "df.createOrReplaceTempView(\"emp_tbl\")\n",
    "\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baead240-8bc3-4874-984e-eea822e9f2be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+\n|empid|Weekend_working_hours|\n+-----+---------------------+\n|   10|            38.550001|\n|   17|             9.400000|\n+-----+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Spark SQL Query to calculate total weekend hours\n",
    "spark.sql(\"\"\"\n",
    "with st1 AS\n",
    "(\n",
    "\tselect *, LAG(id,1) OVER(partition by empid order by id) AS login_time\n",
    "\tFROM\n",
    "\t(select *\n",
    "\t,ROW_NUMBER() OVER(PARTITION BY empid order by id) as rn\n",
    "\tfrom emp_tbl) as tbl\n",
    ")\n",
    "\n",
    "SELECT empid , sum(total_hrs) AS Weekend_working_hours\n",
    "FROM\n",
    "(\n",
    "\tselect *,\n",
    "    DATEDIFF(MINUTE,login_time, id)/60.0 as total_hrs\n",
    "    FROM st1\n",
    "    where rn%2=0\n",
    ") as tbl \n",
    "GROUP BY empid;\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7362b6d-8667-4be1-a818-b186542f478e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+\n|empid|Weekend_working_hours|\n+-----+---------------------+\n|   10|                38.55|\n|   17|                  9.4|\n+-----+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Define window specification for lag and row_number\n",
    "window_spec = Window.partitionBy(\"empid\").orderBy(\"id\")\n",
    "\n",
    "# Create st1 DataFrame with login_time and row_number\n",
    "st1 = df.withColumn(\"login_time\", lag(\"id\", 1).over(window_spec)) \\\n",
    "        .withColumn(\"rn\", row_number().over(window_spec))\n",
    "\n",
    "# Filter out rows with even row number and calculate total hours\n",
    "result = st1.filter(col(\"rn\") % 2 == 0) \\\n",
    "            .withColumn(\"total_hrs\", (unix_timestamp(\"id\") - unix_timestamp(\"login_time\")) / 3600.0) \\\n",
    "            .groupBy(\"empid\") \\\n",
    "            .agg({\"total_hrs\": \"sum\"}) \\\n",
    "            .withColumnRenamed(\"sum(total_hrs)\", \"Weekend_working_hours\")\n",
    "\n",
    "# Show the results\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SQL Interview Question - Find Total Weekend working hours",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
