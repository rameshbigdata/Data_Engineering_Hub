{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47febd33-b0fb-45c8-bb14-a0c0b28fc7be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>age</th><th>city</th></tr></thead><tbody><tr><td>1</td><td>Alice</td><td>30</td><td>New York</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "Alice",
         "30",
         "New York"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# Initialize Spark Session\n",
    "#spark = SparkSession.builder.appName(\"mixedDelimiterExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\"1,Alice\\t30|New York\"]\n",
    "\n",
    "# Creating a DataFrame with a single column\n",
    "df = spark.createDataFrame(data, \"string\")\n",
    "\n",
    "# Custom logic to split the mixed delimiter row\n",
    "split_col = split(df['value'], ',|\\t|\\|')\n",
    "\n",
    "# Creating new columns for each split part\n",
    "df = df.withColumn('id', split_col.getItem(0))\\\n",
    "       .withColumn('name', split_col.getItem(1))\\\n",
    "       .withColumn('age', split_col.getItem(2))\\\n",
    "       .withColumn('city', split_col.getItem(3))\n",
    "\n",
    "# Selecting and showing the result\n",
    "df.select('id', 'name', 'age', 'city').display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04f70700-4376-47dc-985b-924915cc49ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Number</th></tr></thead><tbody><tr><td>3</td></tr><tr><td>6</td></tr><tr><td>9</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3
        ],
        [
         6
        ],
        [
         9
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Number",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#find the missing value from the list \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Find Missing Numbers\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(1,), (2,), (4,), (5,), (7,), (8,), (10,)]\n",
    "df_numbers = spark.createDataFrame(data, [\"Number\"])\n",
    "\n",
    "# Generating a complete sequence DataFrame\n",
    "full_range = spark.range(1, 11).toDF(\"Number\")\n",
    "\n",
    "# Finding missing numbers\n",
    "missing_numbers = full_range.join(df_numbers, \"Number\", \"left_anti\")\n",
    "missing_numbers.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d18dc5ed-3f60-4516-82ab-2e0801c785e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>UserID</th><th>FirstPurchaseDate</th></tr></thead><tbody><tr><td>1</td><td>2023-01-05</td></tr><tr><td>2</td><td>2023-01-03</td></tr><tr><td>3</td><td>2023-01-12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "2023-01-05"
        ],
        [
         2,
         "2023-01-03"
        ],
        [
         3,
         "2023-01-12"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "UserID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "FirstPurchaseDate",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Determine the first purchase date for each user.\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import min\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"FirstPurchaseDate\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "purchase_data = [\n",
    "    Row(UserID=1, PurchaseDate='2023-01-05'),\n",
    "    Row(UserID=1, PurchaseDate='2023-01-10'),\n",
    "    Row(UserID=2, PurchaseDate='2023-01-03'),\n",
    "    Row(UserID=3, PurchaseDate='2023-01-12')\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_purchases = spark.createDataFrame(purchase_data)\n",
    "\n",
    "# Convert PurchaseDate to date type\n",
    "df_purchases = df_purchases.withColumn(\"PurchaseDate\", col(\"PurchaseDate\").cast(\"date\"))\n",
    "\n",
    "# Find first purchase date for each user\n",
    "first_purchase = df_purchases.groupBy(\"UserID\").agg(min(\"PurchaseDate\").alias(\"FirstPurchaseDate\"))\n",
    "\n",
    "# Show results\n",
    "first_purchase.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01c80512-f1ee-405e-aaab-a0aac169d317",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sale_id</th><th>amount</th></tr></thead><tbody><tr><td>1</td><td>100</td></tr><tr><td>2</td><td>150</td></tr><tr><td>3</td><td>150</td></tr><tr><td>4</td><td>200</td></tr><tr><td>5</td><td>150</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         100
        ],
        [
         "2",
         150
        ],
        [
         "3",
         150
        ],
        [
         "4",
         200
        ],
        [
         "5",
         150
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "sale_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Problem: Given a dataset of sales records, identify and replace all missing values in the 'amount' column with the average sales amount.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import mean, col\n",
    "\n",
    "# Initialize Spark Session\n",
    "#spark = SparkSession.builder.appName(\"HandleMissingValues\").getOrCreate()\n",
    "\n",
    "# Sample data for sales - id, amount (with missing values represented by None)\n",
    "sales_data = [(\"1\", 100), (\"2\", 150), (\"3\", None), (\"4\", 200), (\"5\", None)]\n",
    "\n",
    "# Creating DataFrame\n",
    "sales_df = spark.createDataFrame(sales_data, [\"sale_id\", \"amount\"])\n",
    "\n",
    "# Calculate the average sales amount\n",
    "avg_amount = sales_df.na.drop().agg(mean(col(\"amount\"))).first()[0]\n",
    "\n",
    "# Replace missing values with the average amount\n",
    "sales_df_filled = sales_df.na.fill(avg_amount)\n",
    "\n",
    "# Show the result\n",
    "sales_df_filled.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f8a2808-058d-4da4-8161-7149a42215dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Date</th><th>ProductID</th><th>QuantitySold</th><th>7DayAvg</th></tr></thead><tbody><tr><td>2023-01-01</td><td>100</td><td>10</td><td>10.0</td></tr><tr><td>2023-01-02</td><td>100</td><td>15</td><td>12.5</td></tr><tr><td>2023-01-03</td><td>100</td><td>20</td><td>15.0</td></tr><tr><td>2023-01-04</td><td>100</td><td>25</td><td>17.5</td></tr><tr><td>2023-01-05</td><td>100</td><td>30</td><td>20.0</td></tr><tr><td>2023-01-06</td><td>100</td><td>35</td><td>22.5</td></tr><tr><td>2023-01-07</td><td>100</td><td>40</td><td>25.0</td></tr><tr><td>2023-01-08</td><td>100</td><td>45</td><td>30.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2023-01-01",
         100,
         10,
         10.0
        ],
        [
         "2023-01-02",
         100,
         15,
         12.5
        ],
        [
         "2023-01-03",
         100,
         20,
         15.0
        ],
        [
         "2023-01-04",
         100,
         25,
         17.5
        ],
        [
         "2023-01-05",
         100,
         30,
         20.0
        ],
        [
         "2023-01-06",
         100,
         35,
         22.5
        ],
        [
         "2023-01-07",
         100,
         40,
         25.0
        ],
        [
         "2023-01-08",
         100,
         45,
         30.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "ProductID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "QuantitySold",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "7DayAvg",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In a DataFrame df_sales with columns Date, ProductID, and QuantitySold, how would you calculate a 7-day rolling average of QuantitySold for each product?\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"RollingAverageCalculation\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [Row(Date='2023-01-01', ProductID=100, QuantitySold=10),\n",
    "        Row(Date='2023-01-02', ProductID=100, QuantitySold=15),\n",
    "        Row(Date='2023-01-03', ProductID=100, QuantitySold=20),\n",
    "        Row(Date='2023-01-04', ProductID=100, QuantitySold=25),\n",
    "        Row(Date='2023-01-05', ProductID=100, QuantitySold=30),\n",
    "        Row(Date='2023-01-06', ProductID=100, QuantitySold=35),\n",
    "        Row(Date='2023-01-07', ProductID=100, QuantitySold=40),\n",
    "        Row(Date='2023-01-08', ProductID=100, QuantitySold=45)]\n",
    "\n",
    "# Create DataFrame\n",
    "df_sales = spark.createDataFrame(data)\n",
    "\n",
    "# Convert Date string to Date type\n",
    "df_sales = df_sales.withColumn(\"Date\", F.to_date(F.col(\"Date\")))\n",
    "\n",
    "# Window specification for 7-day rolling average\n",
    "windowSpec = Window.partitionBy('ProductID').orderBy('Date').rowsBetween(-6, 0)\n",
    "\n",
    "# Calculating the rolling average\n",
    "rollingAvg = df_sales.withColumn('7DayAvg', F.avg('QuantitySold').over(windowSpec))\n",
    "\n",
    "# Show results\n",
    "rollingAvg.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46efe621-6fdc-40de-9b6f-b8475933cb1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Product</th><th>Month</th><th>Sales</th></tr></thead><tbody><tr><td>Product1</td><td>Jan</td><td>100</td></tr><tr><td>Product1</td><td>Feb</td><td>150</td></tr><tr><td>Product1</td><td>Mar</td><td>200</td></tr><tr><td>Product2</td><td>Jan</td><td>200</td></tr><tr><td>Product2</td><td>Feb</td><td>250</td></tr><tr><td>Product2</td><td>Mar</td><td>300</td></tr><tr><td>Product3</td><td>Jan</td><td>300</td></tr><tr><td>Product3</td><td>Feb</td><td>350</td></tr><tr><td>Product3</td><td>Mar</td><td>400</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Product1",
         "Jan",
         100
        ],
        [
         "Product1",
         "Feb",
         150
        ],
        [
         "Product1",
         "Mar",
         200
        ],
        [
         "Product2",
         "Jan",
         200
        ],
        [
         "Product2",
         "Feb",
         250
        ],
        [
         "Product2",
         "Mar",
         300
        ],
        [
         "Product3",
         "Jan",
         300
        ],
        [
         "Product3",
         "Feb",
         350
        ],
        [
         "Product3",
         "Mar",
         400
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Month",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Problem: Given a dataset of sales records with monthly sales per product, reshape the data to have one row per product-month combination.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Initialize Spark Session\n",
    "#spark = SparkSession.builder.appName(\"DataReshaping\").getOrCreate()\n",
    "\n",
    "# Sample data: Product sales per month\n",
    "data = [(\"Product1\", 100, 150, 200),\n",
    "        (\"Product2\", 200, 250, 300),\n",
    "        (\"Product3\", 300, 350, 400)]\n",
    "\n",
    "# Columns: Product, Sales_Jan, Sales_Feb, Sales_Mar\n",
    "columns = [\"Product\", \"Sales_Jan\", \"Sales_Feb\", \"Sales_Mar\"]\n",
    "\n",
    "# Creating DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Pivoting the DataFrame\n",
    "# This step transforms the data into a long format: Product, Month, Sales\n",
    "pivoted_df = df.selectExpr(\"Product\", \n",
    "                           \"stack(3, 'Jan', Sales_Jan, 'Feb', Sales_Feb, 'Mar', Sales_Mar) as (Month, Sales)\")\n",
    "\n",
    "# Show the result\n",
    "pivoted_df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53f4ada7-1b1b-4685-8f84-5b57c59aaf0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Date</th><th>UniqueVisitors</th></tr></thead><tbody><tr><td>2023-01-01</td><td>2</td></tr><tr><td>2023-01-02</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2023-01-01",
         2
        ],
        [
         "2023-01-02",
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "UniqueVisitors",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find the count of unique visitors to a website per day.\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"UniqueVisitorsPerDay\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "visitor_data = [Row(Date='2023-01-01', VisitorID=101),\n",
    "                Row(Date='2023-01-01', VisitorID=102),\n",
    "                Row(Date='2023-01-01', VisitorID=101),\n",
    "                Row(Date='2023-01-02', VisitorID=103),\n",
    "                Row(Date='2023-01-02', VisitorID=101)]\n",
    "\n",
    "# Create DataFrame\n",
    "df_visitors = spark.createDataFrame(visitor_data)\n",
    "\n",
    "# Count unique visitors per day\n",
    "unique_visitors = df_visitors.groupBy('Date').agg(countDistinct('VisitorID').alias('UniqueVisitors'))\n",
    "\n",
    "# Show results\n",
    "unique_visitors.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be4ccbf5-5011-4c41-8bc5-3fe72aca4c53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>UserID</th><th>Age</th><th>AgeGroup</th></tr></thead><tbody><tr><td>4001</td><td>17</td><td>Youth</td></tr><tr><td>4002</td><td>45</td><td>Adult</td></tr><tr><td>4003</td><td>65</td><td>Senior</td></tr><tr><td>4004</td><td>30</td><td>Adult</td></tr><tr><td>4005</td><td>80</td><td>Senior</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         4001,
         17,
         "Youth"
        ],
        [
         4002,
         45,
         "Adult"
        ],
        [
         4003,
         65,
         "Senior"
        ],
        [
         4004,
         30,
         "Adult"
        ],
        [
         4005,
         80,
         "Senior"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "UserID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "AgeGroup",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How can you use UDFs (User Defined Functions) in PySpark to apply a complex transformation, say, categorizing ages into groups ('Youth', 'Adult', 'Senior')?\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"AgeCategorization\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [Row(UserID=4001, Age=17),\n",
    "        Row(UserID=4002, Age=45),\n",
    "        Row(UserID=4003, Age=65),\n",
    "        Row(UserID=4004, Age=30),\n",
    "        Row(UserID=4005, Age=80)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Define UDF to categorize age\n",
    "def categorize_age(age):\n",
    "    if age < 18:\n",
    "        return 'Youth'\n",
    "    elif age < 60:\n",
    "        return 'Adult'\n",
    "    else:\n",
    "        return 'Senior'\n",
    "\n",
    "age_udf = udf(categorize_age, StringType())\n",
    "\n",
    "# Apply UDF to categorize ages\n",
    "df = df.withColumn('AgeGroup', age_udf(df['Age']))\n",
    "\n",
    "# Show results\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0942b1d0-f0db-48a5-972d-2e9d5484ca87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>MovieID</th><th>AvgRating</th><th>MovieName</th></tr></thead><tbody><tr><td>1</td><td>4.25</td><td>Movie A</td></tr><tr><td>2</td><td>4.25</td><td>Movie B</td></tr><tr><td>3</td><td>4.0</td><td>Movie C</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         4.25,
         "Movie A"
        ],
        [
         2,
         4.25,
         "Movie B"
        ],
        [
         3,
         4.0,
         "Movie C"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "MovieID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "AvgRating",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "MovieName",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#find top 3 movie based on the rating\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Initialize Spark Session\n",
    "#spark = SparkSession.builder.appName(\"TopMovies\").getOrCreate()\n",
    "\n",
    "# Sample DataFrames\n",
    "data_movies = [(1, \"Movie A\"), (2, \"Movie B\"), (3, \"Movie C\"), (4, \"Movie D\"), (5, \"Movie E\")]\n",
    "\n",
    "data_ratings = [(1, 101, 4.5), (1, 102, 4.0), (2, 103, 5.0), \n",
    "                (2, 104, 3.5), (3, 105, 4.0), (3, 106, 4.0), \n",
    "                (4, 107, 3.0), (5, 108, 2.5), (5, 109, 3.0)]\n",
    "\n",
    "columns_movies = [\"MovieID\", \"MovieName\"]\n",
    "columns_ratings = [\"MovieID\", \"UserID\", \"Rating\"]\n",
    "\n",
    "# Creating DataFrames\n",
    "df_movies = spark.createDataFrame(data_movies, columns_movies)\n",
    "df_ratings = spark.createDataFrame(data_ratings, columns_ratings)\n",
    "\n",
    "# Calculating average ratings\n",
    "avg_ratings = df_ratings.groupBy('MovieID').agg(avg('Rating').alias('AvgRating'))\n",
    "\n",
    "# Joining with df_movies to get movie names\n",
    "top_movies = avg_ratings.join(df_movies, 'MovieID').orderBy('AvgRating', ascending=False).limit(3)\n",
    "\n",
    "# Showing the top 3 movies\n",
    "top_movies.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04912e9a-205b-40fb-92e4-9de9187de47e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Spark Join Transformation Generator!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Enter the path to the first text file:  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "\n",
    "def generate_spark_join(dataframe1_name, dataframe2_name, dataframe1_file_path, dataframe2_file_path, join_column):\n",
    "    schema1 = StructType() \\\n",
    "        .add(\"col1\", \"string\") \\\n",
    "        .add(\"col2\", \"string\") \\\n",
    "        .add(\"col3\", \"string\") \\\n",
    "        .add(\"col4\", \"string\") \\\n",
    "        .add(\"col5\", \"string\")\n",
    "\n",
    "    schema2 = StructType() \\\n",
    "        .add(\"col1\", \"string\") \\\n",
    "        .add(\"col2\", \"string\")\n",
    "\n",
    "    spark_code = f\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkJoinTransformation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read text files and create DataFrames\n",
    "{dataframe1_name} = spark.read.option(\"header\", \"true\").csv(\"{dataframe1_file_path}\",schema={schema1})\n",
    "{dataframe2_name} = spark.read.option(\"header\", \"true\").csv(\"{dataframe2_file_path}\",schema={schema2})\n",
    "\n",
    "# Perform join transformation\n",
    "{dataframe1_name}_joined = {dataframe1_name}.join({dataframe2_name}, on='{join_column}', how='inner')\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "{dataframe1_name}_joined.show()\n",
    "\n",
    "\"\"\"\n",
    "    return spark_code\n",
    "\n",
    "def create_github_release(repo_owner, repo_name, access_token, tag_name, release_name, release_body, asset_file_path):\n",
    "    g = Github(access_token)\n",
    "    repo = g.get_repo(f\"{repo_owner}/{repo_name}\")\n",
    "    release = repo.create_git_release(tag=tag_name, name=release_name, message=release_body)\n",
    "    release.upload_asset(asset_file_path)\n",
    "\n",
    "def main():\n",
    "    print(\"Welcome to the Spark Join Transformation Generator!\")\n",
    "\n",
    "    # Read text files and create DataFrames\n",
    "    dataframe1_file_path = input(\"Enter the path to the first text file: \")\n",
    "    dataframe1_name = input(\"Enter the name for the first DataFrame: \")\n",
    "\n",
    "    dataframe2_file_path = input(\"Enter the path to the second text file: \")\n",
    "    dataframe2_name = input(\"Enter the name for the second DataFrame: \")\n",
    "\n",
    "    # Prompt user for join details\n",
    "    join_column = input(\"Enter the column to perform the join on: \")\n",
    "    output_dataframe_name = input(\"Enter the name for the output DataFrame: \")\n",
    "\n",
    "    # Generate Spark code for join transformation\n",
    "    spark_join_code = generate_spark_join(dataframe1_name, dataframe2_name, dataframe1_file_path, dataframe2_file_path, join_column)\n",
    "    print(\"\\nGenerated Spark Code:\")\n",
    "    print(spark_join_code)\n",
    "\n",
    "    # Write Spark code to a temporary text file\n",
    "    with NamedTemporaryFile(mode='w', delete=False) as temp_file:\n",
    "        temp_file.write(spark_join_code)\n",
    "        asset_file_path = temp_file.name\n",
    "\n",
    "    # Deploy the generated code to GitHub as an asset file\n",
    "    deploy_to_github = input(\"Do you want to deploy the generated code to GitHub? (yes/no): \")\n",
    "    if deploy_to_github.lower() == \"yes\":\n",
    "        repo_owner = input(\"Enter the owner of the GitHub repository: \")\n",
    "        repo_name = input(\"Enter the name of the GitHub repository: \")\n",
    "        access_token = input(\"Enter your GitHub personal access token: \")\n",
    "        tag_name = input(\"Enter the tag name for the release: \")\n",
    "        release_name = input(\"Enter the name for the release: \")\n",
    "        release_body = input(\"Enter the body for the release: \")\n",
    "\n",
    "        create_github_release(repo_owner, repo_name, access_token, tag_name, release_name, release_body, asset_file_path)\n",
    "        print(\"Release created successfully on GitHub!\")\n",
    "    else:\n",
    "        print(\"Deployment to GitHub skipped.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "10 PySpark Product Based Interview Questions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
