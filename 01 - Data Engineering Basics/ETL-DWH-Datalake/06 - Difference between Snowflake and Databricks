

𝗦𝗻𝗼𝘄𝗙𝗹𝗮𝗸𝗲:

This is a cloud-based data warehouse as a service. 
They provide ELT support mainly through their COPY command and dedicated schema and file object definitions. 
It allows you to easily spin up multiple independent compute clusters that can operate on the data simultaneously from a single copy of the data. 
In terms of data engineering, they follow the ELT method. 
Nevertheless, they offer good support for 3rd party ETL tools such as fivetran, talend, etc. The software even allows you to install DBT.

𝗗𝗮𝘁𝗮𝗯𝗿𝗶𝗰𝗸𝘀:

Processing power is the main function of data bricks. Spark's core functionality is integrated and it is ideal for ETL loads.
The storage they use is called a data lakehouse, which is similar to a data lake but has relational database functionality. 
This is basically a data lake, but you can run SQL on it, which has become quite popular lately.

You only need to worry about loading your data into snowflake if you have an existing ETL tool like fivetran, talend, tibco, etc.
All of your database infrastructure (paritioning, scalability, indexes, etc.) is being handled for you.

Consider databricks if you don't have an existing ETL tool and your data requires intensive cleaning and has unpredictable data sources and schemas.
Take advantage of the schema on read technique to scale your data.
