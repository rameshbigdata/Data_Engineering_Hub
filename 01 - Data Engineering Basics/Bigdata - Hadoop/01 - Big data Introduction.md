**Why BigData?**

   We can't process huge amount of data classified under above 4V'S(Volume, Velocity, Variety and Veracity) in traditional systems
   To process the data first we need to find a way to store them.
   Where do we store such huge amount of data?
       Can we store and process 1 TB of data if I have storage capacity of 500 GB in single machine? No
       But same 1 TB of can be stored on 10 machines(100 GB each) and process subsequently. This is nothing but distributed system.

       Before Big Data Era -  It is storage and just process in a single machine/server
       After Big Data Era  -  It is distributed storage and distributed processing in cluster(group of machine)
       
   Traditional way of scaling is vertical scaling. Eg: Any Relational Database
   
   For the distributed storage and processing - horizontal scaling(True scaling)
   
   In horizontal scaling, the number of resources increased directly result in increasing the performance.

**Big Data Requirements:**

Store --> Process -> Scale
Store -  store massive amount of data
Process - Process it in a timely manner
Scale - Scale easily as data grows

**Scalability:**
Two ways to build a system 
Monolithic - A powerful system with lot of resources
Distributed - Many smaller systems(nodes) comes together 
                      Each system is node and together is cluster

Monolithic:
 A single powerful server
Hard to add resources after a certain limit

Resources means - 
RAM - 8 GB(Memory)
Hard Disk - 1 TB(Storage)
CPU Quad core (Compute)

Monolithic Architecture -  Vertical Scaling(No True Scaling)
Distributed Architecture - Horizontal Scaling(True scaling)
